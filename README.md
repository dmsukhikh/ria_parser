# RiaParser - система агрегации данных для получения статей с [ria.ru](https://ria.ru/)
Проект представляет собой ETL-систему с API-сервером, для получения полученной информации из базы данных. В частности, проект использует следующие технологии:
- Scrapy - как фреймворк для парсинга веб-страниц
- PostgreSQL - как базу данных
- Flask - как фреймворк для создания API-сервера
## Требования
Для работы проекта необходимы:
- Python версии не ниже 3.1
## Установка
Создайте виртуальное окружение, затем установите в него все зависимости:
```
# Скрипт установки зависимостей на Linux
# На Windows суть такая же
cd project_dir
python -m venv ./venv/
source venv/bin/activate
pip install -r requirements.txt
```
## Использование
Пока реализован только краулер. Запустите краулер при помощь инструментов командной строки Scrapy:
```
scrapy crawl ria
```
Если необходимо вывести всю полученную информацию, используйте флаг ```-O file.jsonl```. Также, можно изменить некоторые настройки парсинга с помощью флага ```-s```. Следующие настройки доступны для изменение:
- ```DOWNLOAD_DELAY``` - время задержки между двумя последовательными запросами к одному домену. Так, если указать ```-s DOWNLOAD_DELAY 2```, то задержка будет **примерно** равна двум секундам. Важно, что краулер настроен рандомизировать время этой задержки, так что время задержки не будет равно в точности двум секундам. По умолчанию равно **1.5 секундам**
- ```CLOSESPIDER_ITEMCOUNT``` - количество статей, которые должен получить краулер во время работы. По умолчанию равно **200**. Получив следующее число, краулер завершает работу. Важно, что фактическое число полученных статей будет больше обозначенного числа *(примерно на 16)*, поскольку будут обработаны все запросы в очереди, даже если указанное флагом количество будет превышено. Чтобы это контролировать, укажите при запуске флаг ```-s CONCURRENT_REQUESTS x```, где **x** - максимальное количество параллельно обрабатываемых запросов
- ```DEPTH_LIMIT``` - глубина следований по сслыкам. По умолчанию равно **4**
